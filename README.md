**Machine learning and Deep learning Project**<br />
**Cecilia Berti s328490, Alessia Manni s331377, Shakti Rathore s328222**<br />

**Paper**<br />
-Not all PDF readers support the GIF format. To have a better visualization of the qualitative results, we suggest using Adobe Acrobat Reader<br />
-Link to the paper:"https://drive.google.com/file/d/1jv0QTO_CJxXTOBoFznfWmVk0pJzaF-qw/view?usp=drive_link"



**Step 4_2**<br />
-In this folder you can find the tests done on the NLQ task, using VSLNet and VSLBase. <br />
-For both we tested Omnivore and EgoVLP features. <br />

**NLQ**<br />
-In this folder you can find the version of the code forked from the original EGO4D repository, modified to implement the VSLbase. For the VSLNet we cloned directyl the original repository in the code when it was needed. <br />


**Step 4_4**<br />
-For this step the variation we decided to implements was the one related to the text encoder, so here you can find also the test done on using the VslNET, also using as encoder GloVe.

**Step 4_4_Extension**<br />
-We decided to implement the second proposal.
<br />

**Qualitative_results**<br />
-In this folder, we have stored additional qualitative results that complement those presented in the paper. Please note that videos cannot be viewed directly from GitHub; you will need to download the notebook.
<br />

**Note**<br />
-In each directory you can find the notebook related to task that we impemented, you can download and run them without any modification, except for Ego4D credentials. The only exception is the notebook we used to compute the predictions score, in this case it is necessary to create a folder "predictions" and upload at least a predictions file.
